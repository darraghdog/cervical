{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import gc, math\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n",
    "\n",
    "from cnnmodels import vgg_std16_model, preprocess_input, create_rect5, load_img, train_generator, test_generator\n",
    "from cnnmodels import identity_block, testcv_generator, conv_block, resnet50_model, save_array, load_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "ROWS, COLS = 224, 224\n",
    "CHECKPOINT_DIR = 'log/checkpoint05/'\n",
    "BATCHSIZE = 128\n",
    "CERV_CLASSES = ['Type_1', 'Type_2', 'Type_3']\n",
    "nb_perClass = int(BATCHSIZE / len(CERV_CLASSES))\n",
    "TRAIN_DIRDM = '../data/cropped/train'\n",
    "TEST_DIRDM = '../data/cropped/test'\n",
    "TRAIN_DIRS = ['../data/original/train', '../data/rfcn_crop/train', '../data/gmm/train']\n",
    "TEST_DIRS = ['../data/original/test', '../data/rfcn_crop/test', '../data/gmm/test']\n",
    "DATA_DIRS = ['../data/original', '../data/rfcn_crop', '../data/gmm']\n",
    "REMOVE_FOLDER = ['../', 'data/', 'cropped/', 'rfcn_crop/', 'gmm/',  'original/']\n",
    "num_class = len(CERV_CLASSES)\n",
    "full = False\n",
    "bags = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_ls = []\n",
    "y_ls = []\n",
    "# Load Raw images in original directory structure format\n",
    "for TRAIN_DIR in TRAIN_DIRS:\n",
    "    for typ in CERV_CLASSES:\n",
    "        for img in os.listdir(os.path.join(TRAIN_DIR, typ)):\n",
    "            if img != '.DS_Store':\n",
    "                img_ls.append(os.path.join(TRAIN_DIR, typ, img))\n",
    "                y_ls.append(typ)\n",
    "# Load rfcn cropped images in original directory structure format\n",
    "for DATA_DIR in DATA_DIRS:\n",
    "    for typ in CERV_CLASSES:\n",
    "        for img in os.listdir(os.path.join(DATA_DIR, typ)):\n",
    "            if img != '.DS_Store':\n",
    "                img_ls.append(os.path.join(DATA_DIR, typ, img))\n",
    "                y_ls.append(typ)\n",
    "# Load Dave's crop images in simplified directory structure\n",
    "for typ in CERV_CLASSES:\n",
    "    for img in os.listdir(os.path.join(TRAIN_DIRDM, typ)):\n",
    "        if img != '.DS_Store':\n",
    "            img_ls.append(os.path.join(TRAIN_DIRDM, typ, img))\n",
    "            y_ls.append(typ)\n",
    "train_all  = pd.DataFrame({'class': y_ls, 'img': img_ls, })[['img', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_ls = []\n",
    "for TEST_DIR in TEST_DIRS:\n",
    "    for img in os.listdir(TEST_DIR):\n",
    "        if img != '.DS_Store':\n",
    "            img_ls.append(os.path.join(TEST_DIR, img))\n",
    "for img in os.listdir(TEST_DIRDM):\n",
    "    if img != '.DS_Store':\n",
    "        img_ls.append(os.path.join(TEST_DIRDM, img))\n",
    "test_df  = pd.DataFrame({'img': img_ls}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001, 1)\n",
      "(31399, 2)\n",
      "                                     img   class\n",
      "0  ../data/original/train/Type_1/779.jpg  Type_1\n",
      "1  ../data/original/train/Type_1/833.jpg  Type_1\n",
      "                                                     img   class\n",
      "31397  ../data/cropped/train/Type_3/4816_T3_additiona...  Type_3\n",
      "31398  ../data/cropped/train/Type_3/4851_T3_additiona...  Type_3\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape)\n",
    "print(train_all.shape)\n",
    "print(train_all.head(2))\n",
    "print(train_all.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_generator(datagen, df):\n",
    "    while 1:\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        batch_y = np.zeros((BATCHSIZE, len(CERV_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby('class', as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = row[0]\n",
    "            typ_class = row[1]\n",
    "            img = Image.open(image_file).resize((ROWS, COLS))\n",
    "            img = img.convert('RGB')\n",
    "            x = np.asarray(img, dtype=K.floatx())\n",
    "            #x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i,CERV_CLASSES.index(typ_class)] = 1\n",
    "            i += 1\n",
    "        #return (batch_x, batch_y)\n",
    "        yield (batch_x.transpose(0, 3, 1, 2), batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Type_2/3498.jpg', 'Type_2/1341.jpg', 'Type_3/6017.jpg', 'Type_2/5629.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train and valid\n",
    "valid_set = pd.read_csv(\"../val_images.csv\", header = None, names = ['img']).img.tolist()\n",
    "#validx = pd.read_csv(\"../cv_split.csv\")\n",
    "#valid_set = []\n",
    "#test_fold = 0\n",
    "#for c, row in validx.iterrows():\n",
    "#    if row['fold'] == test_fold:\n",
    "#        valid_set.append(row['SubDirectory']+'/'+row['file_name'])\n",
    "#    else:\n",
    "#        continue\n",
    "valid_set[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orig_format = []\n",
    "for c, row in train_all.iterrows():\n",
    "    if '../data/cropped' in row['img']:\n",
    "        typ = row['img'].split('/')[-2]\n",
    "        additional = True if 'additional' in row['img'] else False\n",
    "        img = row['img'].split('/')[-1].split('_')[0]\n",
    "        if additional:\n",
    "            orig_format.append(typ+'/'+img+'.jpg')\n",
    "        else:\n",
    "            orig_format.append('train/'+typ+'/'+img+'.jpg')\n",
    "    else:\n",
    "        orig_nm = row['img']\n",
    "        for data_dir in REMOVE_FOLDER:\n",
    "            orig_nm = orig_nm.replace(data_dir, '')\n",
    "        orig_format.append(orig_nm)\n",
    "        \n",
    "train_all['orig_format'] = orig_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>class</th>\n",
       "      <th>orig_format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31394</th>\n",
       "      <td>../data/cropped/train/Type_3/5403_T3_additiona...</td>\n",
       "      <td>Type_3</td>\n",
       "      <td>Type_3/5403.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31395</th>\n",
       "      <td>../data/cropped/train/Type_3/1306_cropped.jpg</td>\n",
       "      <td>Type_3</td>\n",
       "      <td>train/Type_3/1306.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31396</th>\n",
       "      <td>../data/cropped/train/Type_3/4217_T3_additiona...</td>\n",
       "      <td>Type_3</td>\n",
       "      <td>Type_3/4217.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31397</th>\n",
       "      <td>../data/cropped/train/Type_3/4816_T3_additiona...</td>\n",
       "      <td>Type_3</td>\n",
       "      <td>Type_3/4816.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31398</th>\n",
       "      <td>../data/cropped/train/Type_3/4851_T3_additiona...</td>\n",
       "      <td>Type_3</td>\n",
       "      <td>Type_3/4851.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     img   class  \\\n",
       "31394  ../data/cropped/train/Type_3/5403_T3_additiona...  Type_3   \n",
       "31395      ../data/cropped/train/Type_3/1306_cropped.jpg  Type_3   \n",
       "31396  ../data/cropped/train/Type_3/4217_T3_additiona...  Type_3   \n",
       "31397  ../data/cropped/train/Type_3/4816_T3_additiona...  Type_3   \n",
       "31398  ../data/cropped/train/Type_3/4851_T3_additiona...  Type_3   \n",
       "\n",
       "                 orig_format  \n",
       "31394        Type_3/5403.jpg  \n",
       "31395  train/Type_3/1306.jpg  \n",
       "31396        Type_3/4217.jpg  \n",
       "31397        Type_3/4816.jpg  \n",
       "31398        Type_3/4851.jpg  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25164, 3)\n",
      "(6235, 3)\n"
     ]
    }
   ],
   "source": [
    "valid_df = train_all[train_all['orig_format'].isin(valid_set)]\n",
    "if full == True:\n",
    "    train_df = train_all\n",
    "else:\n",
    "    train_df = train_all[~train_all['orig_format'].isin(valid_set)]\n",
    "samples_per_epoch=BATCHSIZE*math.ceil(train_df.groupby('class').size()['Type_2']/nb_perClass)\n",
    "print(train_df.shape)\n",
    "print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val image 0 out of 6235\n",
      "Loading val image 250 out of 6235\n",
      "Loading val image 500 out of 6235\n",
      "Loading val image 750 out of 6235\n",
      "Loading val image 1000 out of 6235\n",
      "Loading val image 1250 out of 6235\n",
      "Loading val image 1500 out of 6235\n",
      "Loading val image 1750 out of 6235\n",
      "Loading val image 2000 out of 6235\n",
      "Loading val image 2250 out of 6235\n",
      "Loading val image 2500 out of 6235\n",
      "Loading val image 2750 out of 6235\n",
      "Loading val image 3000 out of 6235\n",
      "Loading val image 3250 out of 6235\n",
      "Loading val image 3500 out of 6235\n",
      "Loading val image 3750 out of 6235\n",
      "Loading val image 4000 out of 6235\n",
      "Loading val image 4250 out of 6235\n",
      "Loading val image 4500 out of 6235\n",
      "Loading val image 4750 out of 6235\n",
      "Loading val image 5000 out of 6235\n",
      "Loading val image 5250 out of 6235\n",
      "Loading val image 5500 out of 6235\n",
      "Loading val image 5750 out of 6235\n",
      "Loading val image 6000 out of 6235\n"
     ]
    }
   ],
   "source": [
    "# Make our validation set\n",
    "if os.path.exists(\"results/valid_x.dat\"):\n",
    "    valid_x = load_array('results/valid_x.dat')\n",
    "    valid_y = load_array('results/valid_y.dat')\n",
    "else:\n",
    "    l = valid_df.groupby('class').size()\n",
    "    valid_x = np.zeros((valid_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "    valid_y = np.zeros((valid_df.shape[0], len(CERV_CLASSES)), dtype=K.floatx())\n",
    "    i = 0\n",
    "    for index,row in valid_df.iterrows():\n",
    "        if i % 1000 == 0 : print('Loading val image {} out of {}'.format(i, valid_df.shape[0]))\n",
    "        row = row.tolist()\n",
    "        image_file = row[0]\n",
    "        typ_class = row[1]\n",
    "        img = Image.open(image_file).resize((ROWS, COLS))\n",
    "        img = img.convert('RGB')\n",
    "        x = np.asarray(img, dtype=K.floatx())\n",
    "        # x = datagen.random_transform(x)\n",
    "        x = preprocess_input(x)\n",
    "        valid_x[i] = x\n",
    "        valid_y[i,CERV_CLASSES.index(typ_class)] = 1\n",
    "        i += 1\n",
    "    valid_x = valid_x.transpose(0, 3, 1, 2)\n",
    "    save_array('results/valid_x.dat', valid_x)\n",
    "    save_array('results/valid_y.dat', valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_generator(df, datagen, batch_size = BATCHSIZE):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1    \n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0        \n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = row[0]\n",
    "            # typ_class = row[1]\n",
    "            img = Image.open(image_file).resize((ROWS, COLS))\n",
    "            img = img.convert('RGB')\n",
    "            x = np.asarray(img, dtype=K.floatx())\n",
    "            # x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%100 == 0: print(batch_index)\n",
    "        # return (batch_x.transpose(0, 3, 1, 2))\n",
    "        yield(batch_x.transpose(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')        \n",
    "model_checkpoint = ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "# learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)\n",
    "# tensorboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=False, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model creation... \n",
      "Fine tune part 1\n",
      "Epoch 1/2\n",
      "43008/43008 [==============================] - 2385s - loss: 0.8239 - acc: 0.6149 - val_loss: 0.9905 - val_acc: 0.5132\n",
      "Epoch 2/2\n",
      "43008/43008 [==============================] - 2374s - loss: 0.5743 - acc: 0.7703 - val_loss: 0.9421 - val_acc: 0.5670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd27e27790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Model creation... \"\n",
    "nb_epoch = 2\n",
    "model = resnet50_model(ROWS, COLS, channel, num_class)\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[-3:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Start Fine-tuning\n",
    "print \"Fine tune part 1\"\n",
    "model.fit_generator(train_generator(train_datagen, train_df),\n",
    "          nb_epoch=nb_epoch,\n",
    "          samples_per_epoch=samples_per_epoch, #40864\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y),\n",
    "          #callbacks=[early_stopping, model_checkpoint],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tune part 1A\n",
      "Epoch 1/5\n",
      "43008/43008 [==============================] - 2379s - loss: 0.3923 - acc: 0.8596 - val_loss: 0.9801 - val_acc: 0.5917\n",
      "Epoch 2/5\n",
      "43008/43008 [==============================] - 2378s - loss: 0.2598 - acc: 0.9162 - val_loss: 1.0353 - val_acc: 0.6026\n",
      "Epoch 3/5\n",
      "43008/43008 [==============================] - 2380s - loss: 0.1684 - acc: 0.9514 - val_loss: 1.0956 - val_acc: 0.6205\n",
      "Epoch 4/5\n",
      "43008/43008 [==============================] - 2381s - loss: 0.1090 - acc: 0.9727 - val_loss: 1.1943 - val_acc: 0.6218\n",
      "Epoch 5/5\n",
      "43008/43008 [==============================] - 2379s - loss: 0.0700 - acc: 0.9846 - val_loss: 1.2421 - val_acc: 0.6269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd2ad4c5d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "nb_epoch = 5\n",
    "print \"Fine tune part 1A\"\n",
    "model.fit_generator(train_generator(train_datagen, train_df),\n",
    "          nb_epoch=nb_epoch,\n",
    "          samples_per_epoch=samples_per_epoch, #50000,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y),\n",
    "          #callbacks=[early_stopping, model_checkpoint],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tune part 2\n",
      "Epoch 1/5\n",
      "43008/43008 [==============================] - 2379s - loss: 0.0477 - acc: 0.9916 - val_loss: 1.3103 - val_acc: 0.6361\n",
      "Epoch 2/5\n",
      "43008/43008 [==============================] - 2381s - loss: 0.0344 - acc: 0.9941 - val_loss: 1.4003 - val_acc: 0.6411\n",
      "Epoch 3/5\n",
      "43008/43008 [==============================] - 2378s - loss: 0.0236 - acc: 0.9968 - val_loss: 1.4666 - val_acc: 0.6319\n",
      "Epoch 4/5\n",
      "43008/43008 [==============================] - 2374s - loss: 0.0179 - acc: 0.9979 - val_loss: 1.4841 - val_acc: 0.6428\n",
      "Epoch 5/5\n",
      "43008/43008 [==============================] - 2375s - loss: 0.0142 - acc: 0.9982 - val_loss: 1.5596 - val_acc: 0.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd2add8410>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Resnet50\n",
    "# fine tuning\n",
    "# 164 conv5c+top\n",
    "# 142 conv5+top\n",
    "# 80 conv4+conv5+top\n",
    "# 38 conv3+conv4+conv5+top\n",
    "start_layer = 164\n",
    "\n",
    "model.optimizer.lr = 1e-6\n",
    "for layer in model.layers[start_layer:]:\n",
    "    layer.trainable = True\n",
    "nb_epoch = 5\n",
    "print \"Fine tune part 2\"\n",
    "model.fit_generator(train_generator(train_datagen, df=train_df),\n",
    "          nb_epoch=nb_epoch,\n",
    "          samples_per_epoch=samples_per_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y),\n",
    "          #callbacks=[model_checkpoint, early_stopping], # , \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 'log/checkpoint05/weights.000-1.0398.hdf5'\n",
      "Update 'log/checkpoint05/weights.000-1.0591.hdf5'\n",
      "Update 'log/checkpoint05/weights.001-1.0032.hdf5'\n",
      "Update 'log/checkpoint05/weights.001-1.0186.hdf5'\n",
      "Update 'log/checkpoint05/weights.002-1.0116.hdf5'\n",
      "Update 'log/checkpoint05/weights.003-1.0083.hdf5'\n"
     ]
    }
   ],
   "source": [
    "# Hack to solve issue on model loading : https://github.com/fchollet/keras/issues/4044\n",
    "import glob\n",
    "import h5py\n",
    "model_files = sorted(glob.glob(CHECKPOINT_DIR + '*.hdf5'))\n",
    "for model_file in model_files:\n",
    "    print(\"Update '{}'\".format(model_file))\n",
    "    with h5py.File(model_file, 'a') as f:\n",
    "        if 'optimizer_weights' in f.keys():\n",
    "            del f['optimizer_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag the predictions from a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(CHECKPOINT_DIR+'*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "min_id = np.array(val_losses).argsort()[:bags].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop the lowest val losses and get a prediction for each\n",
    "test_preds_ls = []\n",
    "for index in min_id:\n",
    "    print('Loading model from checkpoints file ' + files[index])\n",
    "    test_model = load_model(files[index])\n",
    "    test_model_name = files[index].split('/')[-2][-1:]+'_'+files[index].split('/')[-1]\n",
    "    test_preds_ls.append(test_model.predict_generator(test_generator(test_df, train_datagen), \n",
    "                                         val_samples = test_df.shape[0])) \n",
    "    del test_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = sum(test_preds_ls)/len(test_preds_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sub = pd.DataFrame(test_preds, columns=CERV_CLASSES)\n",
    "test_sub['image_name'] = test_df['img'].str.split('/').apply(lambda x: x[-1])\n",
    "test_sub = test_sub[['image_name'] + CERV_CLASSES ]\n",
    "test_sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if full:\n",
    "    subm_name = '../sub/sub_dara_full_resnet_all_5xbag_' + timestr + '.csv' #'.csv.gz'\n",
    "else:\n",
    "    subm_name = '../sub/sub_dara_part_resnet_all_5xbag_' + timestr + '.csv' #'.csv.gz'\n",
    "    \n",
    "#test_sub.to_csv(subm_name, index=False)#, compression='gzip')\n",
    "#FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
