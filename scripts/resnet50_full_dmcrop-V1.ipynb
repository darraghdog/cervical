{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import gc, math\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n",
    "\n",
    "from cnnmodels import vgg_std16_model, preprocess_input, create_rect5, load_img, train_generator, test_generator\n",
    "from cnnmodels import identity_block, testcv_generator, conv_block, resnet50_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "ROWS, COLS = 224, 224\n",
    "CHECKPOINT_DIR = 'log/checkpoint04/'\n",
    "BATCHSIZE = 32\n",
    "CERV_CLASSES = ['Type_1', 'Type_2', 'Type_3']\n",
    "nb_perClass = int(BATCHSIZE / len(CERV_CLASSES))\n",
    "TRAIN_DIR = '../data/cropped/train'\n",
    "TEST_DIR = '../data/cropped/test'\n",
    "DATA_DIR = '../data'\n",
    "num_class = len(CERV_CLASSES)\n",
    "full = True\n",
    "bags = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_ls = []\n",
    "y_ls = []\n",
    "for typ in CERV_CLASSES:\n",
    "    for img in os.listdir(os.path.join(TRAIN_DIR, typ)):\n",
    "        if img != '.DS_Store':\n",
    "            img_ls.append(os.path.join(TRAIN_DIR, typ, img))\n",
    "            y_ls.append(typ)\n",
    "# for typ in CERV_CLASSES:\n",
    "#     for img in os.listdir(os.path.join(DATA_DIR, typ)):\n",
    "#         if img != '.DS_Store':\n",
    "#             img_ls.append(os.path.join(DATA_DIR, typ, img))\n",
    "#             y_ls.append(typ)\n",
    "train_all  = pd.DataFrame({'class': y_ls, 'img': img_ls, })[['img', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_ls = []\n",
    "for img in os.listdir(TEST_DIR):\n",
    "    if img != '.DS_Store':\n",
    "        img_ls.append(os.path.join(TEST_DIR, img))\n",
    "test_df  = pd.DataFrame({'img': img_ls}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>../data/cropped/train/Type_3/4217_T3_additiona...</td>\n",
       "      <td>Type_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8210</th>\n",
       "      <td>../data/cropped/train/Type_3/4816_T3_additiona...</td>\n",
       "      <td>Type_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8211</th>\n",
       "      <td>../data/cropped/train/Type_3/4851_T3_additiona...</td>\n",
       "      <td>Type_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    img   class\n",
       "8209  ../data/cropped/train/Type_3/4217_T3_additiona...  Type_3\n",
       "8210  ../data/cropped/train/Type_3/4816_T3_additiona...  Type_3\n",
       "8211  ../data/cropped/train/Type_3/4851_T3_additiona...  Type_3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_generator(datagen, df):\n",
    "    while 1:\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        batch_y = np.zeros((BATCHSIZE, len(CERV_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby('class', as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = row[0]\n",
    "            typ_class = row[1]\n",
    "            img = Image.open(image_file).resize((ROWS, COLS))\n",
    "            img = img.convert('RGB')\n",
    "            x = np.asarray(img, dtype=K.floatx())\n",
    "            #x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i,CERV_CLASSES.index(typ_class)] = 1\n",
    "            i += 1\n",
    "        #return (batch_x, batch_y)\n",
    "        yield (batch_x.transpose(0, 3, 1, 2), batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type_2/3498.jpg', 'Type_2/1341.jpg', 'Type_3/6017.jpg', 'Type_2/5629.jpg']\n",
      "['Type_2/1315.jpg', 'train/Type_2/162.jpg', 'train/Type_2/270.jpg', 'train/Type_2/382.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Split into train and valid\n",
    "valid_set = pd.read_csv(\"../val_images.csv\", header = None, names = ['img']).img.tolist()\n",
    "print(valid_set[-4:])\n",
    "print(valid_set[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type_1/5975.jpg', 'Type_1/3526.jpg', 'Type_1/2402.jpg', 'Type_1/4943.jpg', 'Type_1/6414.jpg', 'Type_1/3832.jpg', 'Type_1/5769.jpg', 'Type_1/6182.jpg', 'Type_1/888.jpg', 'Type_1/1189.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Get the same validation set in terms of the new naming for the crops\n",
    "trnidx = []\n",
    "for c, row in train_all.iterrows():\n",
    "    typ = row['img'].split('/')[-2]\n",
    "    additional = True if 'additional' in row['img'] else False\n",
    "    img = row['img'].split('/')[-1].split('_')[0]\n",
    "    if additional:\n",
    "        trnidx.append(typ+'/'+img+'.jpg')\n",
    "    else:\n",
    "        trnidx.append('train/'+typ+'/'+img+'.jpg')\n",
    "print trnidx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8212, 2)\n",
      "(1642, 2)\n"
     ]
    }
   ],
   "source": [
    "valid_df = train_all[[f in valid_set for f in trnidx]]\n",
    "if full == True:\n",
    "    train_df = train_all\n",
    "else:\n",
    "    train_df = train_all[[f not in valid_set for f in trnidx]]\n",
    "samples_per_epoch=BATCHSIZE*math.ceil(train_df.groupby('class').size()['Type_2']/nb_perClass)\n",
    "print(train_df.shape)\n",
    "print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make our validation set\n",
    "l = valid_df.groupby('class').size()\n",
    "valid_x = np.zeros((valid_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "valid_y = np.zeros((valid_df.shape[0], len(CERV_CLASSES)), dtype=K.floatx())\n",
    "i = 0\n",
    "for index,row in valid_df.iterrows():\n",
    "    row = row.tolist()\n",
    "    image_file = row[0]\n",
    "    typ_class = row[1]\n",
    "    img = Image.open(image_file).resize((ROWS, COLS))\n",
    "    img = img.convert('RGB')\n",
    "    x = np.asarray(img, dtype=K.floatx())\n",
    "    # x = datagen.random_transform(x)\n",
    "    x = preprocess_input(x)\n",
    "    valid_x[i] = x\n",
    "    valid_y[i,CERV_CLASSES.index(typ_class)] = 1\n",
    "    i += 1\n",
    "valid_x = valid_x.transpose(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_generator(df, datagen, batch_size = BATCHSIZE):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1    \n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0        \n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = row[0]\n",
    "            # typ_class = row[1]\n",
    "            img = Image.open(image_file).resize((ROWS, COLS))\n",
    "            img = img.convert('RGB')\n",
    "            x = np.asarray(img, dtype=K.floatx())\n",
    "            # x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%100 == 0: print(batch_index)\n",
    "        # return (batch_x.transpose(0, 3, 1, 2))\n",
    "        yield(batch_x.transpose(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')        \n",
    "model_checkpoint = ModelCheckpoint(filepath=CHECKPOINT_DIR+'weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "# learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)\n",
    "# tensorboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=False, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model creation... \n",
      "Fine tune part 1\n",
      "Epoch 1/2\n",
      "13856/13888 [============================>.] - ETA: 1s - loss: 0.9147 - acc: 0.5323Epoch 00000: val_loss improved from inf to 0.88167, saving model to log/checkpoint04/weights.000-0.8817.hdf5\n",
      "13888/13888 [==============================] - 738s - loss: 0.9150 - acc: 0.5324 - val_loss: 0.8817 - val_acc: 0.5719\n",
      "Epoch 2/2\n",
      "13856/13888 [============================>.] - ETA: 1s - loss: 0.7374 - acc: 0.6390Epoch 00001: val_loss improved from 0.88167 to 0.74638, saving model to log/checkpoint04/weights.001-0.7464.hdf5\n",
      "13888/13888 [==============================] - 736s - loss: 0.7372 - acc: 0.6390 - val_loss: 0.7464 - val_acc: 0.6730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e8dff91d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Model creation... \"\n",
    "nb_epoch = 2\n",
    "model = resnet50_model(ROWS, COLS, channel, num_class)\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[-3:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Start Fine-tuning\n",
    "print \"Fine tune part 1\"\n",
    "model.fit_generator(train_generator(train_datagen, train_df),\n",
    "          nb_epoch=nb_epoch,\n",
    "          samples_per_epoch=samples_per_epoch, #50000,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y),\n",
    "          callbacks=[early_stopping, model_checkpoint],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tune part 1A\n",
      "Epoch 1/6\n",
      "13856/13888 [============================>.] - ETA: 1s - loss: 0.6171 - acc: 0.6964Epoch 00000: val_loss improved from 0.74638 to 0.60794, saving model to log/checkpoint04/weights.000-0.6079.hdf5\n",
      "13888/13888 [==============================] - 735s - loss: 0.6169 - acc: 0.6966 - val_loss: 0.6079 - val_acc: 0.7594\n",
      "Epoch 2/6\n",
      "13856/13888 [============================>.] - ETA: 1s - loss: 0.5085 - acc: 0.7600Epoch 00001: val_loss improved from 0.60794 to 0.49189, saving model to log/checkpoint04/weights.001-0.4919.hdf5\n",
      "13888/13888 [==============================] - 735s - loss: 0.5083 - acc: 0.7602 - val_loss: 0.4919 - val_acc: 0.8136\n",
      "Epoch 3/6\n",
      "13856/13888 [============================>.] - ETA: 1s - loss: 0.4109 - acc: 0.8100Epoch 00002: val_loss improved from 0.49189 to 0.39567, saving model to log/checkpoint04/weights.002-0.3957.hdf5\n",
      "13888/13888 [==============================] - 735s - loss: 0.4107 - acc: 0.8102 - val_loss: 0.3957 - val_acc: 0.8709\n",
      "Epoch 4/6\n",
      "13856/13888 [============================>.] - ETA: 1s - loss: 0.3291 - acc: 0.8427Epoch 00003: val_loss improved from 0.39567 to 0.30392, saving model to log/checkpoint04/weights.003-0.3039.hdf5\n",
      "13888/13888 [==============================] - 735s - loss: 0.3288 - acc: 0.8430 - val_loss: 0.3039 - val_acc: 0.9147\n",
      "Epoch 5/6\n",
      "13856/13888 [============================>.] - ETA: 1s - loss: 0.2588 - acc: 0.8746Epoch 00004: val_loss improved from 0.30392 to 0.23967, saving model to log/checkpoint04/weights.004-0.2397.hdf5\n",
      "13888/13888 [==============================] - 735s - loss: 0.2588 - acc: 0.8746 - val_loss: 0.2397 - val_acc: 0.9428\n",
      "Epoch 6/6\n",
      "13856/13888 [============================>.] - ETA: 1s - loss: 0.2016 - acc: 0.8974Epoch 00005: val_loss improved from 0.23967 to 0.18803, saving model to log/checkpoint04/weights.005-0.1880.hdf5\n",
      "13888/13888 [==============================] - 735s - loss: 0.2015 - acc: 0.8975 - val_loss: 0.1880 - val_acc: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e9138fc50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "nb_epoch = 6\n",
    "print \"Fine tune part 1A\"\n",
    "model.fit_generator(train_generator(train_datagen, train_df),\n",
    "          nb_epoch=nb_epoch,\n",
    "          samples_per_epoch=samples_per_epoch, #50000,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y),\n",
    "          callbacks=[early_stopping, model_checkpoint],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from keras.models import load_model, model_from_json\n",
    "#model = load_model('log/checkpoint01weights.003-0.7728.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 'log/checkpoint04/weights.000-0.6079.hdf5'\n",
      "Update 'log/checkpoint04/weights.000-0.8817.hdf5'\n",
      "Update 'log/checkpoint04/weights.000-1.0465.hdf5'\n",
      "Update 'log/checkpoint04/weights.001-0.4919.hdf5'\n",
      "Update 'log/checkpoint04/weights.001-0.7464.hdf5'\n",
      "Update 'log/checkpoint04/weights.001-0.9793.hdf5'\n",
      "Update 'log/checkpoint04/weights.001-1.0158.hdf5'\n",
      "Update 'log/checkpoint04/weights.002-0.3957.hdf5'\n",
      "Update 'log/checkpoint04/weights.002-0.9685.hdf5'\n",
      "Update 'log/checkpoint04/weights.003-0.3039.hdf5'\n",
      "Update 'log/checkpoint04/weights.004-0.2397.hdf5'\n",
      "Update 'log/checkpoint04/weights.005-0.1880.hdf5'\n"
     ]
    }
   ],
   "source": [
    "# Hack to solve issue on model loading : https://github.com/fchollet/keras/issues/4044\n",
    "import glob\n",
    "import h5py\n",
    "model_files = sorted(glob.glob(CHECKPOINT_DIR + '*.hdf5'))\n",
    "for model_file in model_files:\n",
    "    print(\"Update '{}'\".format(model_file))\n",
    "    with h5py.File(model_file, 'a') as f:\n",
    "        if 'optimizer_weights' in f.keys():\n",
    "            del f['optimizer_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag the predictions from a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(CHECKPOINT_DIR+'*')\n",
    "# there is apparently overfitting on the later epochs so exclude the epochs where we unfroze the top layers\n",
    "files = [f for f in files if float(f.split('-')[-1][:-5])>0.26]\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "min_id = np.array(val_losses).argsort()[:bags].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints file log/checkpoint04/weights.003-0.3039.hdf5\n",
      "0\n",
      "Loading model from checkpoints file log/checkpoint04/weights.002-0.3957.hdf5\n",
      "0\n",
      "Loading model from checkpoints file log/checkpoint04/weights.001-0.4919.hdf5\n",
      "0\n",
      "Loading model from checkpoints file log/checkpoint04/weights.000-0.6079.hdf5\n",
      "0\n",
      "Loading model from checkpoints file log/checkpoint04/weights.001-0.7464.hdf5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Loop the lowest val losses and get a prediction for each\n",
    "test_preds_ls = []\n",
    "for index in min_id:\n",
    "    print('Loading model from checkpoints file ' + files[index])\n",
    "    test_model = load_model(files[index])\n",
    "    test_model_name = files[index].split('/')[-2][-1:]+'_'+files[index].split('/')[-1]\n",
    "    test_preds_ls.append(test_model.predict_generator(test_generator(test_df, train_datagen), \n",
    "                                         val_samples = test_df.shape[0])) \n",
    "    del test_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = sum(test_preds_ls)/len(test_preds_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>Type_1</th>\n",
       "      <th>Type_2</th>\n",
       "      <th>Type_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505.jpg</td>\n",
       "      <td>0.291503</td>\n",
       "      <td>0.690975</td>\n",
       "      <td>0.017521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>472.jpg</td>\n",
       "      <td>0.584193</td>\n",
       "      <td>0.168960</td>\n",
       "      <td>0.246847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262.jpg</td>\n",
       "      <td>0.227657</td>\n",
       "      <td>0.593099</td>\n",
       "      <td>0.179245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name    Type_1    Type_2    Type_3\n",
       "0    505.jpg  0.291503  0.690975  0.017521\n",
       "1    472.jpg  0.584193  0.168960  0.246847\n",
       "2    262.jpg  0.227657  0.593099  0.179245"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub = pd.DataFrame(test_preds, columns=CERV_CLASSES)\n",
    "test_sub['image_name'] = test_df['img'].str.split('/').apply(lambda x: x[-1])\n",
    "test_sub = test_sub[['image_name'] + CERV_CLASSES ]\n",
    "test_sub['image_name'] = test_sub['image_name'].str.replace('_cropped', '')\n",
    "test_sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../sub/sub_dara_full_resnet_dmcrop_5xbag_20170513.csv' target='_blank'>../sub/sub_dara_full_resnet_dmcrop_5xbag_20170513.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/cervical/sub/sub_dara_full_resnet_dmcrop_5xbag_20170513.csv"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from IPython.lib.display import FileLink\n",
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "if full:\n",
    "    subm_name = '../sub/sub_dara_full_resnet_dmcrop_5xbag_' + timestr + '.csv' #'.csv.gz'\n",
    "else:\n",
    "    subm_name = '../sub/sub_dara_part_resnet_dmcrop_5xbag_' + timestr + '.csv' #'.csv.gz'\n",
    "    \n",
    "test_sub.to_csv(subm_name, index=False)#, compression='gzip')\n",
    "FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
